# Llama2 fine tune

æœ¬æ–‡ä»‹ç»ä¸ºä»€ä¹ˆæŒ‡ä»¤è°ƒæ•´æœ‰æ•ˆï¼Œå¹¶å­¦ä¹ å¦‚ä½•åœ¨åˆ›å»ºè‡ªå·±çš„ Llama2 æ¨¡å‹ã€‚

## LLM å¾®è°ƒä»‹ç»

LLMï¼ˆLarge Language Modelsï¼‰æ˜¯åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚å°± Llama2 è€Œè¨€ï¼Œé™¤äº†å…¶ 2 ä¸‡äº¿ token çš„é•¿åº¦å¤–ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒé›†çš„ç»„æˆäº†è§£ç”šå°‘ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBERTï¼ˆ2018ï¼‰ä»…åœ¨ BookCorpusï¼ˆ8 äº¿ä¸ªå•è¯ï¼‰å’Œè‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆ 25 äº¿ä¸ªå•è¯ï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚æ ¹æ®ç»éªŒï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ï¼Œå­˜åœ¨è®¸å¤šç¡¬ä»¶é—®é¢˜ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šä¿¡æ¯ï¼Œæˆ‘å»ºè®®é˜…è¯» Meta å…³äº [OPT-175B](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf) æ¨¡å‹çš„é¢„è®­ç»ƒçš„æ—¥å¿—ã€‚

å½“é¢„è®­ç»ƒå®Œæˆåï¼Œåƒ Llama2 è¿™æ ·çš„è‡ªå›å½’æ¨¡å‹å¯ä»¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª tokenã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ„å‘³ç€å®ƒä»¬ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•å›ç­”æŒ‡ä»¤ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¥ä½¿å®ƒä»¬çš„å›ç­”ä¸äººç±»çš„æœŸæœ›ç›¸ä¸€è‡´ã€‚æœ‰ä¸¤ç§ä¸»è¦çš„å¾®è°ƒæŠ€æœ¯ï¼š

- Supervised Fine-Tuningï¼ˆSFTï¼‰ï¼šæ¨¡å‹åœ¨ä¸€ç»„æŒ‡ä»¤å’Œå›ç­”çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®ƒè°ƒæ•´ LLM ä¸­çš„æƒé‡ï¼Œä»¥æœ€å°åŒ–ç”Ÿæˆçš„ç­”æ¡ˆä¸çœŸå®å›ç­”æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚

- Reinforcement Learning from Human Feedbackï¼ˆRLHFï¼‰ï¼šæ¨¡å‹é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å¹¶æ¥æ”¶åé¦ˆæ¥è¿›è¡Œå­¦ä¹ ã€‚å®ƒä»¬è¢«è®­ç»ƒä»¥æœ€å¤§åŒ–å¥–åŠ±ä¿¡å·ï¼ˆä½¿ç”¨ [PPO](https://arxiv.org/abs/1707.06347)ï¼‰ï¼Œè¿™ä¸ªå¥–åŠ±ä¿¡å·é€šå¸¸æºè‡ªäººç±»å¯¹æ¨¡å‹è¾“å‡ºçš„è¯„ä¼°ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­å­¦ä¹ åˆ°æ›´å¤æ‚å’Œå¾®å¦™çš„äººç±»åå¥½ï¼Œä½†ä¹Ÿæ›´å…·æŒ‘æˆ˜æ€§ï¼Œå®ç°èµ·æ¥æ›´ä¸ºå›°éš¾ã€‚äº‹å®ä¸Šï¼Œå®ƒéœ€è¦å¯¹å¥–åŠ±ç³»ç»Ÿè¿›è¡Œç²¾å¿ƒè®¾è®¡ï¼Œå¹¶ä¸”å¯¹äººç±»åé¦ˆçš„è´¨é‡å’Œä¸€è‡´æ€§éå¸¸æ•æ„Ÿã€‚æœªæ¥çš„ä¸€ä¸ªå¯èƒ½çš„æ›¿ä»£æ–¹æ³•æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆ[Direct Preference Optimization](https://arxiv.org/abs/2305.18290) DPOï¼‰ç®—æ³•ï¼Œå®ƒåœ¨ SFT æ¨¡å‹ä¸Šç›´æ¥è¿›è¡Œåå¥½å­¦ä¹ ã€‚

åœ¨æœ¬æ–‡ï¼Œæˆ‘ä»¬å°†è¿›è¡Œ SFTï¼Œä½†è¿™å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆå¾®è°ƒä¸€å¼€å§‹å°±æœ‰æ•ˆå‘¢ï¼Ÿæ­£å¦‚ [Orca è®ºæ–‡](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/orca.html)ä¸­å¼ºè°ƒçš„é‚£æ ·ï¼Œæˆ‘ä»¬çš„ç†è§£æ˜¯å¾®è°ƒåˆ©ç”¨äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„çŸ¥è¯†ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæ¨¡å‹ä»æœªè§è¿‡æ‚¨æ„Ÿå…´è¶£çš„æ•°æ®ç±»å‹ï¼Œå¾®è°ƒå‡ ä¹æ²¡æœ‰å¸®åŠ©ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒå­¦ä¹ è¿‡è¿™äº›çŸ¥è¯†ï¼ŒSFT å¯ä»¥è¡¨ç°å‡ºæé«˜çš„æ€§èƒ½ã€‚

ä¾‹å¦‚ï¼Œ[LIMA è®ºæ–‡](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/lima.html)å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¯¹åªæœ‰ 1,000 ä¸ªé«˜è´¨é‡æ ·æœ¬è¿›è¡Œ 65 äº¿å‚æ•°çš„ LLaMAï¼ˆv1ï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¶…è¶Š GPT-3ï¼ˆDaVinci003ï¼‰ã€‚è¾¾åˆ°è¿™ç§æ€§èƒ½æ°´å¹³çš„å…³é”®æ˜¯æŒ‡ä»¤æ•°æ®é›†çš„è´¨é‡ï¼Œå› æ­¤å¾ˆå¤šå·¥ä½œéƒ½é›†ä¸­åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ä¸Šï¼ˆå¦‚ evol-instructã€Orca æˆ– phi-1ï¼‰ã€‚è¯·æ³¨æ„ï¼ŒLLM çš„å¤§å°ï¼ˆ65bï¼Œè€Œä¸æ˜¯ 13b æˆ– 7bï¼‰å¯¹äºæœ‰æ•ˆåˆ©ç”¨é¢„å…ˆå­˜åœ¨çš„çŸ¥è¯†ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚

ä¸æ•°æ®è´¨é‡ç›¸å…³çš„å¦ä¸€ä¸ªé‡è¦ç‚¹æ˜¯æç¤ºè¯æ¨¡æ¿(prompt template)ã€‚æç¤ºè¯ç”±ç±»ä¼¼çš„å…ƒç´ ç»„æˆï¼šç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰ç”¨äºæŒ‡å¯¼æ¨¡å‹ï¼Œç”¨æˆ·æç¤ºè¯ï¼ˆå¿…éœ€ï¼‰ç”¨äºç»™å‡ºæŒ‡ä»¤ï¼Œé™„åŠ è¾“å…¥ï¼ˆå¯é€‰ï¼‰ç”¨äºè€ƒè™‘ï¼Œä»¥åŠæ¨¡å‹çš„å›ç­”ï¼ˆå¿…éœ€ï¼‰ã€‚å°± Llama2 è€Œè¨€ï¼Œä½¿ç”¨äº†ä»¥ä¸‹çš„èŠå¤©æ¨¡å‹æ¨¡æ¿ï¼š

```
<s>[INST] <<SYS>>
System prompt
<</SYS>>

User prompt [/INST] Model answer </s>
```

è¿˜æœ‰å…¶ä»–æ¨¡æ¿ï¼Œæ¯”å¦‚ Alpaca å’Œ Vicuna çš„æ¨¡æ¿ï¼Œè¿™äº›æ¨¡æ¿çš„å½±å“å¹¶ä¸æ˜¯å¾ˆæ¸…æ¥šã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ–°æ ¼å¼åŒ–æˆ‘ä»¬çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥ç¬¦åˆ Llama2 çš„æ¨¡æ¿ã€‚æœ¬æ–‡ä½¿ç”¨äº†ä¼˜ç§€çš„ [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) æ•°æ®é›†å®Œæˆäº†è¿™ä¸€æ­¥éª¤ã€‚æ‚¨å¯ä»¥åœ¨ Hugging Face ä¸Šæ‰¾åˆ°å®ƒï¼Œåç§°ä¸º [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è€Œä¸æ˜¯èŠå¤©æ¨¡å‹ï¼Œæ‰€ä»¥è¿™ä¸€æ­¥éª¤æ˜¯å¯é€‰çš„ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯åŸºç¡€çš„ Llama2 æ¨¡å‹è€Œä¸æ˜¯èŠå¤©ç‰ˆæœ¬ï¼Œåˆ™ä¸éœ€è¦éµå¾ªç‰¹å®šçš„æç¤ºæ¨¡æ¿ã€‚

## å¦‚ä½•å¾®è°ƒ Llama2

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹å…·æœ‰ 7B å‚æ•°çš„ Llama2 æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚Llama2-7b çš„æƒé‡æœ‰ 7B Ã— 2byte = 14GBï¼ˆä½¿ç”¨ FP16 ç¼–ç ï¼‰ã€‚å¾®è°ƒè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‰å‘æ¿€æ´»ä¼šäº§ç”Ÿçš„é¢å¤–çš„æ˜¾å­˜å¼€é”€ï¼Œè¿™è®©æˆ‘ä»¬åœ¨å…·å¤‡ 24G æˆ–è€…æ›´ä½æ˜¾å­˜çš„ GPU æ— æ³•è®­ç»ƒã€‚å› æ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆparameter-efficient fine-tuning PEFTï¼‰æŠ€æœ¯ï¼Œå¦‚ LoRA æˆ– QLoRAã€‚

ä¸ºäº†å¤§å¹…å‡å°‘ VRAM çš„ä½¿ç”¨é‡ï¼Œæˆ‘ä»¬å¿…é¡»ä»¥ 4-bit ç²¾åº¦è¿›è¡Œå¾®è°ƒï¼Œå› æ­¤æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨ QLoRAã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­çš„ transformersã€accelerateã€peftã€trl å’Œ bitsandbytes åº“ã€‚ä»¥ä¸‹æ˜¯åŸºäº Younes Belkada çš„ [GitHub Gist](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da) çš„ä»£ç ï¼Œæˆ‘ä»¬å°†åœ¨å…¶ä¸­è¿›è¡Œæ“ä½œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®‰è£…å¹¶åŠ è½½è¿™äº›åº“ã€‚


```
!pip install accelerate peft bitsandbytes transformers trl tensorboard
```

```
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer
```

### æ‰§è¡Œ base æ¨¡å‹

ç„¶åæˆ‘ä»¬å…ˆæ‰§è¡Œä¸€ä¸‹ base æ¨¡å‹ llama-2-7b-chat-hfï¼Œå°è¯•é—®å‡ ä¸ªé—®é¢˜ï¼Œçœ‹ä¼šå¾—åˆ°ä»€ä¹ˆå›å¤ï¼Œåé¢è·Ÿå¾®è°ƒè¿‡çš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚

```
model_name = "NousResearch/llama-2-7b-chat-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map={"": 0},
    trust_remote_code=True,
    torch_dtype=torch.bfloat16).eval()

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

prompt = "What is a large language model?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])
```

```
<s>[INST] What is a large language model? [/INST]  A large language model is a type of artificial intelligence (AI) model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. The model is designed to learn the patterns and structures of language by analyzing a large amount of text data, such as books, articles, and websites.

The size of a large language model can vary, but it is typically measured in terms of the number of parameters or the amount of training data used to train the model. Some common types of large language models include:

1. Transformer-based models: These models are based on the Transformer architecture, which was introduced in 2017 and has since become a popular choice for natural language processing tasks. Transformer-based models are trained on large amounts of text data and use self-attention mechanisms to learn the relationships between
```

```
prompt = "Who is Leonardo Da Vinci?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])
```

```
<s>[INST] Who is Leonardo Da Vinci? [/INST]  Leonardo da Vinci (1452-1519) was a true Renaissance man - an Italian polymath, artist, inventor, engineer, and scientist. He is widely considered one of the most diversely talented individuals to have ever lived.

Da Vinci was born in Vinci, Italy, and was the illegitimate son of a notary named Ser Piero and a peasant woman named Caterina Buti. Despite his humble beginnings, he went on to become one of the most renowned artists of the Renaissance, known for his iconic paintings such as the Mona Lisa and The Last Supper.

In addition to his artistic pursuits, Da Vinci was also a prolific inventor and engineer. He designed flying machines, armored vehicles, and submarines, as well as invent
```
```
prompt = "I have 25 coins each weighing 1tr.oz in a closed cylindrical tube weighing 6.5g. The tube is 10.6cm long and has a diameter of .8in. What is the SI-unit density of the tube, disregarding any air inside it?"

å›ç­”ï¼š
<s>[INST] I have 25 coins each weighing 1tr.oz in a closed cylindrical tube weighing 6.5g. The tube is 10.6cm long and has a diameter of .8in. What is the SI-unit density of the tube, disregarding any air inside it? [/INST]  To find the SI-unit density of the tube, we need to divide the mass of the tube by its volume.

The volume of the tube can be calculated using the formula for the volume of a cylinder:

V = Ï€rÂ²h

where r is the radius of the tube and h is its height (in this case, the length of the tube).

The radius of the tube is half the diameter, so:

r = 0.8in / 2 = 0.4in
```


```
prompt = "ä½ æ˜¯ä¸€ä½å®‹è¯å­¦è€…ï¼Œå¸®åŠ©æˆ‘å­¦ä¹ ä¸€äº›å®‹è¯ç›¸å…³çš„çŸ¥è¯†ã€‚"

å›ç­”ï¼š
<s>[INST] ä½ æ˜¯ä¸€ä½å®‹è¯å­¦è€…ï¼Œå¸®åŠ©æˆ‘å­¦ä¹ ä¸€äº›å®‹è¯ç›¸å…³çš„çŸ¥è¯†ã€‚ [/INST]  Ah, a fellow seeker of knowledge! ğŸ˜Š As a Song Dynasty scholar, I'd be delighted to share some insights and tips on learning about Song Dynasty-related knowledge.

1. Start with the basics: Begin by familiarizing yourself with the key historical events, cultural practices, and literary works of the Song Dynasty (960-1279 AD). This will provide a solid foundation for your studies.
2. Read the classics: The Song Dynasty was a period of great literary achievement, and many of the works from this era are still studied and admired today. Some of the most famous works include "The Classic of Poetry,"
```

åé¢ä¸¤æ¡æ˜¯ä»æ•°æ®é›† [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k) æ‘˜é€‰å‡ºæ¥çš„ã€‚

```
<s>[INST] I have 25 coins each weighing 1tr.oz in a closed cylindrical tube weighing 6.5g. The tube is 10.6cm long and has a diameter of .8in. What is the SI-unit density of the tube, disregarding any air inside it? [/INST] 1 troy ounce = 31.1 g Total mass = 25 coins * 31.1 g/coin + 6.5 g = 784.0 g 1 inch = 2.54 cm Volume = L * pi * r^2 = 10.6 cm * 3.14 * (0.8 in * 2.54 cm/in)^2 = 33.28 cm * (2.03 cm)^2 = 137.4 cm^3 Density = mass/volume = (784.0 g) / (137.4 cm^3) = 5.71 g/cm^3 </s><s>[INST] Based on coin density, what material are these coins made of? [/INST] The density of just the coins (disregarding the tube) is about 5.659 g/cm^3. Tin has a density of approximately 5.765 g/cm^3, making it a likely metal for the coins to be made of. </s>
```

```
<s>[INST] ä½ æ˜¯ä¸€ä½å®‹è¯å­¦è€…ï¼Œå¸®åŠ©æˆ‘å­¦ä¹ ä¸€äº›å®‹è¯ç›¸å…³çš„çŸ¥è¯†ã€‚ [/INST] å½“ç„¶å¯ä»¥ï¼å®‹è¯æ˜¯ä¸­å›½æ–‡å­¦å²ä¸Šçš„ä¸€éƒ¨åˆ†ï¼Œå‡ºç°åœ¨å…¬å…ƒ960å¹´è‡³1279å¹´çš„å®‹ä»£ã€‚å®ƒæ˜¯ä¸€ç§å¤å…¸è¯—æ­Œå½¢å¼ï¼Œå…·æœ‰ç‹¬ç‰¹çš„è‰ºæœ¯é­…åŠ›ã€‚å­¦ä¹ å®‹è¯ä¸ä»…èƒ½å¤Ÿäº†è§£ä¸­å›½å¤ä»£æ–‡åŒ–ï¼Œè¿˜èƒ½é¢†ç•¥åˆ°å½“æ—¶è¯—äººä»¬ä¸°å¯Œçš„æƒ…æ„Ÿä¸ä¼˜ç¾çš„æ–‡å­—ã€‚ä¸‹é¢æ˜¯ä¸€äº›å…³äºå®‹è¯çš„åŸºæœ¬çŸ¥è¯†ï¼š 1. è¯ç‰Œåï¼šå®‹è¯æœ‰ä¸¥æ ¼çš„æ ¼å¾‹è¦æ±‚ï¼Œæ¯é¦–è¯éƒ½æœ‰ä¸€ä¸ªç‰¹å®šçš„è¯ç‰Œåã€‚è¯ç‰Œåè§„å®šäº†è¯çš„å­—æ•°ã€å¹³ä»„ã€éŸµè„šç­‰è¦ç´ ã€‚å¸¸è§çš„è¯ç‰Œåæœ‰ï¼šæ°´è°ƒæ­Œå¤´ã€æµªæ·˜æ²™ã€æ¸”å®¶å‚²ç­‰ã€‚ 2. ä½œè€…ï¼šå®‹è¯æœ‰å¾ˆå¤šè‘—åçš„è¯äººï¼Œå…¶ä¸­æœ€ä¸ºäººä»¬ç†ŸçŸ¥çš„æ˜¯è‹è½¼ã€è¾›å¼ƒç–¾ã€ææ¸…ç…§å’ŒæŸ³æ°¸ç­‰ã€‚ä»–ä»¬çš„ä½œå“ä»£è¡¨äº†å®‹è¯çš„æœ€é«˜è‰ºæœ¯æˆå°±ã€‚ 3. é¢˜æï¼šå®‹è¯çš„é¢˜æä¸°å¯Œå¤šæ ·ï¼ŒåŒ…æ‹¬å’å²æŠ’æ€€ã€å±±æ°´ç”°å›­ã€è¾¹å¡æˆæ¥¼ã€å®«å»·ç”Ÿæ´»ã€èŠ‚ä»¤é£ä¿—ç­‰ã€‚è¯äººä»¬ä»¥å…¶ç‹¬ç‰¹çš„å®¡ç¾è§†è§’ï¼Œå°†ç”Ÿæ´»ä¸­çš„ç‚¹æ»´åŒ–ä¸ºè¯—ç¯‡ï¼Œå±•ç°äº†å½“æ—¶çš„ç¤¾ä¼šé£è²Œã€‚ 4. éŸµå¾‹ï¼šä¸å¤è¯—ç›¸æ¯”ï¼Œå®‹è¯çš„éŸµå¾‹æ›´åŠ ä¸°å¯Œå¤šæ ·ã€‚é™¤äº†å¸¸è§„çš„å¹³ä»„å’Œå¯¹ä»—å¤–ï¼Œè¿˜æœ‰è®¸å¤šç‰¹æ®Šçš„æŠ€å·§ï¼Œå¦‚å€’è£…ã€æŠ¼éŸµç­‰ã€‚è¿™äº›æŠ€å·§ä½¿å¾—å®‹è¯çš„è¯­è¨€æ›´åŠ ä¼˜ç¾ï¼Œå¯Œæœ‰éŸ³ä¹æ€§ã€‚ 5. è‰ºæœ¯ç‰¹è‰²ï¼šå®‹è¯æ³¨é‡å†…åœ¨æƒ…æ„Ÿçš„è¡¨è¾¾ï¼Œä»¥â€œå©‰çº¦â€å’Œâ€œè±ªæ”¾â€ä¸ºä¸»è¦è‰ºæœ¯é£æ ¼ã€‚å©‰çº¦æ´¾ä»¥ææ¸…ç…§ä¸ºä»£è¡¨ï¼Œè¯—è¯å­—é‡Œè¡Œé—´æµéœ²å‡ºæ‚ æ‰¬ã€æŠ’æƒ…çš„ç¾ï¼›è±ªæ”¾æ´¾ä»¥è‹è½¼ä¸ºä»£è¡¨ï¼Œä½œå“ä¸­å±•ç°å‡ºè±ªè¿ˆã€å¥”æ”¾çš„ç²¾ç¥ã€‚ äº†è§£ä»¥ä¸ŠåŸºæœ¬çŸ¥è¯†åï¼Œä½ å¯ä»¥å°è¯•é˜…è¯»ä¸€äº›å®‹è¯ä½œå“ï¼Œä½“ä¼šå…¶ä¸­çš„ç¾æ„Ÿã€‚åŒæ—¶ï¼Œä½ è¿˜å¯ä»¥å­¦ä¹ ä¸€äº›æœ‰å…³å®‹è¯çš„è¯„è®ºä¸ç ”ç©¶ï¼Œä»¥ä¾¿æ›´æ·±å…¥åœ°ç†è§£å’Œæ¬£èµè¿™ä¸€è‰ºæœ¯å½¢å¼ã€‚å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼ </s>
```

### å¾®è°ƒ base æ¨¡å‹

è®©æˆ‘ä»¬ç¨å¾®è°ˆè°ˆæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´çš„å‚æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æƒ³è¦åŠ è½½ä¸€ä¸ª llama-2-7b-chat-hf æ¨¡å‹ï¼Œå¹¶åœ¨ mlabonne/guanaco-llama2-1kï¼ˆ1,000 ä¸ªæ ·æœ¬ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å°†ç”Ÿæˆæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹ llama-2-7b-miniguanacoã€‚å¦‚æœæ‚¨å¯¹å¦‚ä½•åˆ›å»ºè¿™ä¸ªæ•°æ®é›†æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹è¿™ä¸ªnotebookã€‚éšæ„æ›´æ”¹å®ƒï¼šHugging Face Hubä¸Šæœ‰è®¸å¤šå¾ˆå¥½çš„æ•°æ®é›†ï¼Œå¦‚databricks/databricks-dolly-15kã€‚

QLoRA å°†ä½¿ç”¨ 64 rank å’Œ 16 lora_alphaã€‚æˆ‘ä»¬å°†ä½¿ç”¨ NF4 ç±»å‹ç›´æ¥ä»¥ 4-bit ç²¾åº¦åŠ è½½ Llama2 æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œ 1 ä¸ª epoch çš„è®­ç»ƒã€‚è¦è·å–æœ‰å…³å…¶ä»–å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥é˜… TrainingArgumentsã€PeftModel å’Œ SFTTrainer çš„æ–‡æ¡£ã€‚

```
# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/llama-2-7b-chat-hf"

# The instruction dataset to use
dataset_name = "mlabonne/guanaco-llama2-1k"

# Fine-tuned model name
new_model = "llama-2-7b-miniguanaco"

################################################################################
# QLoRA parameters
################################################################################

# LoRA attention dimension
lora_r = 64

# Alpha parameter for LoRA scaling
lora_alpha = 16

# Dropout probability for LoRA layers
lora_dropout = 0.1

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################

# Output directory where the model predictions and checkpoints will be stored
output_dir = "./results"

# Number of training epochs
num_train_epochs = 1

# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False

# Batch size per GPU for training
per_device_train_batch_size = 4

# Batch size per GPU for evaluation
per_device_eval_batch_size = 4

# Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 1

# Enable gradient checkpointing
gradient_checkpointing = True

# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3

# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4

# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001

# Optimizer to use
optim = "paged_adamw_32bit"

# Learning rate schedule (constant a bit better than cosine)
lr_scheduler_type = "constant"

# Number of training steps (overrides num_train_epochs)
max_steps = -1

# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03

# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True

# Save checkpoint every X updates steps
save_steps = 25

# Log every X updates steps
logging_steps = 25

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}
```

ç”Ÿæˆæ•°æ®é›† `mlabonne/guanaco-llama2-1k` çš„ä»£ç ï¼š

```
from datasets import load_dataset
import re

# Load the dataset
dataset = load_dataset('timdettmers/openassistant-guanaco')

# Shuffle the dataset and slice it
dataset = dataset['train'].shuffle(seed=42).select(range(1000))

# Define a function to transform the data
def transform_conversation(example):
    conversation_text = example['text']
    segments = conversation_text.split('###')

    reformatted_segments = []

    # Iterate over pairs of segments
    for i in range(1, len(segments) - 1, 2):
        human_text = segments[i].strip().replace('Human:', '').strip()

        # Check if there is a corresponding assistant segment before processing
        if i + 1 < len(segments):
            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()

            # Apply the new template
            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')
        else:
            # Handle the case where there is no corresponding assistant segment
            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')

    return {'text': ''.join(reformatted_segments)}


# Apply the transformation
transformed_dataset = dataset.map(transform_conversation)

transformed_dataset.push_to_hub("guanaco-llama2-1k")
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥åŠ è½½æ‰€æœ‰å†…å®¹å¹¶å¼€å§‹å¾®è°ƒè¿‡ç¨‹ã€‚

- é¦–å…ˆï¼Œæˆ‘ä»¬è¦åŠ è½½æˆ‘ä»¬å®šä¹‰çš„æ•°æ®é›†ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å·²ç»ç»è¿‡é¢„å¤„ç†ï¼Œä½†é€šå¸¸åœ¨è¿™é‡Œæ‚¨å°†é‡æ–°æ ¼å¼åŒ–æç¤ºï¼Œè¿‡æ»¤æ‰ä¸è‰¯æ–‡æœ¬ï¼Œåˆå¹¶å¤šä¸ªæ•°æ®é›†ç­‰ã€‚
- ç„¶åï¼Œæˆ‘ä»¬é…ç½® bitsandbytes ä»¥è¿›è¡Œ 4-bit é‡åŒ–ã€‚
- æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åœ¨ç›¸åº”çš„ tokenizer ä¸Šä»¥ 4-bit ç²¾åº¦åŠ è½½ Llama2 æ¨¡å‹ã€‚
- æœ€åï¼Œæˆ‘ä»¬åŠ è½½ QLoRA çš„é…ç½®ã€å¸¸è§„è®­ç»ƒå‚æ•°ï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ SFTTrainerã€‚è®­ç»ƒç°åœ¨å¯ä»¥å¼€å§‹äº†ï¼

```
# Load dataset (you can process it here)
dataset = load_dataset(dataset_name, split="train")

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•åœ¨ TensorBoard ä¸ŠæŸ¥çœ‹å›¾è¡¨ï¼š



ä¸ºäº†ç¡®ä¿æ¨¡å‹çš„è¡Œä¸ºæ˜¯æ­£ç¡®çš„é€šå¸¸éœ€è¦æ›´è¯¦å°½çš„è¯„ä¼°ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæµæ°´çº¿æ¥æé—®è¯¸å¦‚ â€œWhat is a large language model?â€ çš„é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œè¿™é‡Œéœ€è¦æ ¼å¼åŒ–è¾“å…¥ä»¥åŒ¹é… Llama2 çš„æç¤ºè¯æ¨¡æ¿ã€‚

```
# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

# Run text generation pipeline with our next model
prompt = "What is a large language model?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])
```

```
<s>[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.

Large language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.

Large language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given
```

æ¨¡å‹è¾“å‡ºå¦‚ä¸‹çš„åº”ç­”ï¼š

```
A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.

Large language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.

Large language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given
```

æ ¹æ®ç»éªŒï¼Œå¯¹äºåªæœ‰ 7B å‚æ•°çš„æ¨¡å‹æ¥è¯´ï¼Œå®ƒçš„è¿è´¯æ€§éå¸¸å¥½ã€‚æ‚¨å¯ä»¥è¿›è¡Œå°è¯•ï¼Œå¹¶ä» [BigBench-Hard](https://github.com/suzgunmirac/BIG-Bench-Hard) ç­‰è¯„ä¼°æ•°æ®é›†ä¸­é—®æ›´éš¾çš„é—®é¢˜ã€‚Guanaco æ˜¯ä¸€ä¸ªäº§ç”Ÿè¿‡é«˜è´¨é‡æ¨¡å‹çš„ä¼˜ç§€æ•°æ®é›†ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ [mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2) åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ª Llama2 æ¨¡å‹ã€‚

ç°åœ¨æˆ‘ä»¬å¦‚ä½•å­˜å‚¨æˆ‘ä»¬çš„æ–°æ¨¡å‹ llama-2-7b-miniguanaco å‘¢ï¼Ÿæˆ‘ä»¬éœ€è¦å°† LoRA çš„æƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ®æˆ‘æ‰€çŸ¥ï¼Œæ²¡æœ‰ç›´æ¥çš„æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼šæˆ‘ä»¬éœ€è¦ä»¥ FP16 ç²¾åº¦é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ peft åº“æ¥åˆå¹¶æ‰€æœ‰å†…å®¹ã€‚

```
# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
```

æˆ‘ä»¬å·²ç»åˆå¹¶äº†æƒé‡å¹¶é‡æ–°åŠ è½½äº† tokenizerã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ° Hugging Face Hub ä»¥ä¿å­˜æˆ‘ä»¬çš„æ¨¡å‹ã€‚

```
!huggingface-cli login

model.push_to_hub(new_model, use_temp_dir=False)
tokenizer.push_to_hub(new_model, use_temp_dir=False)
```

æ‚¨ç°åœ¨å¯ä»¥åƒä» Hub åŠ è½½ä»»ä½•å…¶ä»– Llama2 æ¨¡å‹ä¸€æ ·ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚æ‚¨ä¹Ÿå¯ä»¥é‡æ–°åŠ è½½å®ƒå¹¶ä½¿ç”¨å¦ä¸€ä¸ªæ•°æ®é›†æ¥è¿›ä¸€æ­¥å¾®è°ƒã€‚

å»ºè®®ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„è„šæœ¬æ¥æ‰§è¡Œå¾®è°ƒï¼Œåœ¨[è¿™é‡Œ](https://gist.github.com/mlabonne/8eb9ad60c6340cb48a17385c68e3b1a5)å¯ä»¥æ‰¾åˆ°ç±»ä¼¼çš„ä»£ç ã€‚


## æ€»ç»“

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•å¯¹ Llama2-7b æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä»‹ç»äº†ä¸€äº›å…³äº LLM è®­ç»ƒå’Œå¾®è°ƒçš„å¿…è¦èƒŒæ™¯çŸ¥è¯†ï¼Œä»¥åŠä¸æŒ‡ä»¤æ•°æ®é›†ç›¸å…³çš„é‡è¦è€ƒè™‘å› ç´ ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä½¿ç”¨åŸç”Ÿæç¤ºè¯æ¨¡æ¿å’Œè‡ªå®šä¹‰å‚æ•°å¯¹ Llama2 æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚


## å‚è€ƒ
- https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html
- https://www.datacamp.com/tutorial/fine-tuning-llama-2